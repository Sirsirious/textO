{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy import displacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "import requests, json\n",
    "\n",
    "nlp = spacy.load('pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('data/Raw/quinhentas_perguntas.csv', sep=',', header=0)\n",
    "answers = pd.read_csv('data/Raw/quinhentas_respostas.csv', sep=\",\", header=0)\n",
    "corpus['resposta'] = answers['resposta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1127 Current runtime: 63.9435 seconds\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from multiprocessing import Manager, Pool\n",
    "import wikipedia\n",
    "import time\n",
    "start_time = time.time()\n",
    "from threading import RLock\n",
    "lock = RLock()\n",
    "\n",
    "\n",
    "def searchRelation(idx, terms, page, word, nonexistant, disambiguate = True):\n",
    "    try:\n",
    "        page = wikipedia.page(page)\n",
    "        #This will move trough the pages linked to this one.\n",
    "        g = dict()\n",
    "        g[word] = set()\n",
    "        for token in page.links:\n",
    "            tLemma = lemmatizer(token, \"NOUN\")[0]\n",
    "            if tLemma in terms:\n",
    "                g[word].add(tLemma)\n",
    "        lock.acquire()\n",
    "        graph.update(g)\n",
    "        clear_output()\n",
    "        print(idx, \"Current runtime: %.4f seconds\"% (time.time() - start_time))\n",
    "        lock.release()\n",
    "    except wikipedia.DisambiguationError as e:\n",
    "        #Ambiguous - need to try another\n",
    "        if disambiguate == True:\n",
    "            for option in e.options:\n",
    "                searchRelation(idx, g, terms, option, word, nonexistant, False)\n",
    "    except wikipedia.PageError as e:\n",
    "        #No page with ID\n",
    "        #An automated attempt would be to use some dictionary to try \"related\" words and have a score for them.\n",
    "        nonexistant.append(word)\n",
    "    except KeyError as e:\n",
    "        #print(e)\n",
    "        nonexistant.append(word)\n",
    "    except ConnectionError as e:\n",
    "        #print(e)\n",
    "        nonexistant.append(word)\n",
    "    \n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "wikipedia.set_lang('pt')\n",
    "terms = pd.read_csv('data/TreatedTerms/tfidfterms.csv')\n",
    "bigrams = pd.read_csv('data/TreatedTerms/bigrams.csv')\n",
    "trigrams = pd.read_csv('data/TreatedTerms/trigrams.csv')\n",
    "bigramset = set(bigrams[bigrams.columns[1]])\n",
    "trigramset = set(trigrams[trigrams.columns[1]])\n",
    "terms = pd.concat([terms, bigrams], ignore_index=True)\n",
    "terms = pd.concat([terms, trigrams], ignore_index=True)\n",
    "termset = set(terms[terms.columns[1]])\n",
    "termset = termset.union(trigramset)\n",
    "termset = termset.union(bigramset)\n",
    "\n",
    "#Parallel Data Structures \n",
    "manager = Manager()\n",
    "#Ensure access to parallel modules, lock is used to deal with mess\n",
    "global graph\n",
    "graph = manager.dict()\n",
    "termset = manager.dict(dict.fromkeys(termset, 0))\n",
    "nonexistant = manager.list()\n",
    "\n",
    "#Parallel!!\n",
    "pool = Pool(processes=50)\n",
    "results = [pool.apply_async(searchRelation, args=(idx, termset, row[1], row[1], nonexistant)) for idx, row in terms.iterrows()]\n",
    "\n",
    "#print(\"Final Runtime: %.4f seconds --- Finished!\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(nonexistant)\n",
    "newlist = ['enxerto', 'decumbens', 'aveia', 'inseminação', 'lactação', 'secreção', 'aglomeração', 'ração', 'ordenha', 'cisto folicular', 'quarto mamário', 'digestível total', 'inimigo natural', 'silo aéreo', 'adubo', 'dieta completa', 'reserva corporal', 'babesia', 'índice reprodutivo', 'exigência nutricional', 'Tobiatã', 'solo ácido', 'espécie arbórea']\n",
    "newtermset = set(terms[terms.columns[1]])\n",
    "newtermset = newtermset.union(trigramset)\n",
    "newtermset = newtermset.union(bigramset)\n",
    "newtermset = newtermset.union(set(newlist))\n",
    "managedNewTermSet = manager.dict(dict.fromkeys(newtermset, 0))\n",
    "results = [pool.apply_async(searchRelation, args=(1, termset, word, nonexistant)) for word in newlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_list = []\n",
    "for key in graph:\n",
    "    if(len(key) < 2):\n",
    "        continue\n",
    "    for rel in graph[key]:\n",
    "        if(len(rel) < 2):\n",
    "            continue\n",
    "        #print(key)\n",
    "        row_list.append([key, rel])\n",
    "result = pd.DataFrame(row_list, columns=['from', 'to'])\n",
    "result.to_csv('data/Networks/Wikipedia/wikipediaTerms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1139 Current runtime: 62.9103 seconds\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from multiprocessing import Manager, Pool\n",
    "import wikipedia\n",
    "import time\n",
    "start_time = time.time()\n",
    "from threading import RLock\n",
    "lock = RLock()\n",
    "\n",
    "\n",
    "def searchRelation(idx, terms, page, word, nonexistant, disambiguate = True):\n",
    "    try:\n",
    "        page = wikipedia.page(page)\n",
    "        #This will move trough the pages linked to this one.\n",
    "        g = dict()\n",
    "        g[word] = set()\n",
    "        for token in page.links:\n",
    "            tLemma = lemmatizer(token, \"NOUN\")[0]\n",
    "            if tLemma in terms:\n",
    "                g[word].add(tLemma)\n",
    "        lock.acquire()\n",
    "        graph.update(g)\n",
    "        clear_output()\n",
    "        print(idx, \"Current runtime: %.4f seconds\"% (time.time() - start_time))\n",
    "        lock.release()\n",
    "    except wikipedia.DisambiguationError as e:\n",
    "        #Ambiguous - need to try another\n",
    "        if disambiguate == True:\n",
    "            for option in e.options:\n",
    "                searchRelation(idx, g, terms, option, word, nonexistant, False)\n",
    "    except wikipedia.PageError as e:\n",
    "        #No page with ID\n",
    "        #An automated attempt would be to use some dictionary to try \"related\" words and have a score for them.\n",
    "        nonexistant.append(word)\n",
    "    except KeyError as e:\n",
    "        #print(e)\n",
    "        nonexistant.append(word)\n",
    "    except ConnectionError as e:\n",
    "        #print(e)\n",
    "        nonexistant.append(word)\n",
    "    \n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "wikipedia.set_lang('pt')\n",
    "terms = pd.read_csv('data/TreatedTerms/tfidfterms.csv')\n",
    "questionterms = pd.read_csv('data/TreatedTerms/wordsFromQuestionsFiltered.csv')\n",
    "bigrams = pd.read_csv('data/TreatedTerms/bigrams.csv')\n",
    "trigrams = pd.read_csv('data/TreatedTerms/trigrams.csv')\n",
    "qtermset = set(questionterms[questionterms.columns[1]])\n",
    "bigramset = set(bigrams[bigrams.columns[1]])\n",
    "trigramset = set(trigrams[trigrams.columns[1]])\n",
    "terms = pd.concat([terms, bigrams], ignore_index=True)\n",
    "terms = pd.concat([terms, trigrams], ignore_index=True)\n",
    "termset = set(terms[terms.columns[1]])\n",
    "termset = termset.union(trigramset)\n",
    "termset = termset.union(bigramset)\n",
    "termset = termset.union(qtermset)\n",
    "\n",
    "#Parallel Data Structures \n",
    "manager = Manager()\n",
    "#Ensure access to parallel modules, lock is used to deal with mess\n",
    "global graph\n",
    "graph = manager.dict()\n",
    "termset = manager.dict(dict.fromkeys(termset, 0))\n",
    "nonexistant = manager.list()\n",
    "\n",
    "#Parallel!!\n",
    "pool = Pool(processes=50)\n",
    "results = [pool.apply_async(searchRelation, args=(idx, termset, row[1], row[1], nonexistant)) for idx, row in terms.iterrows()]\n",
    "\n",
    "#print(\"Final Runtime: %.4f seconds --- Finished!\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_list = []\n",
    "for key in graph:\n",
    "    if(len(key) < 2):\n",
    "        continue\n",
    "    for rel in graph[key]:\n",
    "        if(len(rel) < 2):\n",
    "            continue\n",
    "        #print(key)\n",
    "        row_list.append([key, rel])\n",
    "result = pd.DataFrame(row_list, columns=['from', 'to'])\n",
    "result.to_csv('data/Networks/Wikipedia/wikipediaTermsWithQuestionTerms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['instalaçõe', 'provado', 'identificar', 'inseminaçõe', 'aaveia', 'lactaçõe', 'secreçõe', 'alimentada', 'melhorar', 'essenciais', 'oideal', 'aglomeraçõe', 'substituir', 'ocorte', 'obter', 'apresentam', 'quais', 'raçõe', 'relacionado', 'tricross', 'envolvido', 'causada', 'sincronização', 'ordenhada', 'melhoria', 'devido', 'adistribuição', 'aplicar', 'orufião', 'revacinar', 'recomendaçõe', 'apresentar', 'degradada', 'sobreordenha', 'portanto', 'demora', 'cistos foliculares', 'quer dizer', 'polpa cítrica', 'cochos cobertos', 'reação inflamatória', 'digestíveis totais', 'quartos mamários', 'núcleo Moet', 'silos aéreos', 'adubo químico', 'desaleitamento precoce', 'reservas corporais', 'práticas agronômicas', 'dietas completas', 'abrigos individuais', 'valores genéticos', 'exame andrológico', 'agente causador', 'períodos prolongados', 'desmama precoce', 'dejeto líquido', 'seleção genômica', 'danos causados', 'ração concentrada', 'cocho coberto', 'mesma coisa', 'culturas anuais', 'estresse calórico', 'primeiras 24horas', 'longos períodos', 'primeiros jatos', 'índices reprodutivos', 'exigências nutricionais', 'espaçamento recomendados', 'esterco semissólido', 'primeiros 100dias', 'sólidos totais', 'Nessa situação', 'sêmen sexado', 'infecções uterinas', 'reter todas', 'Caso haja', 'cultivar Tobiatã', 'quantos meses', 'quantos dias', 'Nesses casos', 'Além disso', 'monta natural', 'grandes infestações', 'pequenas quantidades', 'condição corporal', 'cultivar Pioneiro', 'Existe algum', 'monta controlada', 'relação touro/vaca', 'solos férteis', 'desempenho produtivo', 'touros melhoradores', 'esterco líquido', 'indicadores técnicos', 'zebu leiteiro', 'solos ácidos', 'deve-se descartar', 'embriões Pive', 'Podem-se usar', 'espécies arbóreas', 'relação custo/benefício', 'agentes causadores', 'microrganismos causadores', 'touros provados', 'causar danos', 'pastejo rotativo', 'para vacas leiteiras']\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
