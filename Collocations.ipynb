{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import pandas as pd\n",
    "\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "\n",
    "\n",
    "\n",
    "sentences = Text8Corpus(datapath('/home/tiago_duque/Área de Trabalho/Dissertação/textO/data/Raw/500perguntasgadoleite.txt'))\n",
    "phrases = Phrases(sentences, min_count=2, threshold=1)\n",
    "termlist = pd.read_csv('/home/tiago_duque/Área de Trabalho/Dissertação/textO/data/TreatedTerms/tfidfterms.csv')\n",
    "\n",
    "sent = termlist['0'].tolist()\n",
    "termlist = [term for term in termlist if len(term) >2]\n",
    "#print(sent)\n",
    "\n",
    "bigram = Phraser(phrases)\n",
    "#sent = [u'gado', u'leite', u'brachiaria', u'decumbens']\n",
    "for sent in sentences:\n",
    "    tokens_ = bigram[sent]\n",
    "    for token in tokens_:\n",
    "        if '_' in token:\n",
    "            words = token.split(\"_\")\n",
    "            w0l = lemmatizer(words[0],'NOUN')[0]\n",
    "            w1l = lemmatizer(words[1],'ADJ')[0]\n",
    "            if (words[0] in termlist or w0l in termlist) and (words[1] in termlist or w1l in termlist):\n",
    "                print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deve ser; devem ser; pode ser; dos animais; sal mineral; Além disso;\n",
      "podem ser; matéria seca; inseminação artificial; acordo com; azevém\n",
      "anual; condição corporal; glândula mamária; ser feita; melhoramento\n",
      "genético; valor nutritivo; raças europeias; Brachiaria decumbens;\n",
      "células somáticas; médico veterinário\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import Text\n",
    "from nltk import word_tokenize\n",
    "\n",
    "stwords = set(stopwords.words('portuguese'))\n",
    "\n",
    "root = 'data/Raw/quinhentasCorpus/'\n",
    "reader = PlaintextCorpusReader(root,'.*','.txt')\n",
    "raw = reader.raw()\n",
    "tokens = word_tokenize(raw)\n",
    "text = Text(tokens)\n",
    "text.collocations()\n",
    "termlist = pd.read_csv('/home/tiago_duque/Área de Trabalho/Dissertação/textO/data/TreatedTerms/tfidfterms.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "import pandas as pd\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "tfinder = TrigramCollocationFinder.from_words(tokens)\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "ignored_words = nltk.corpus.stopwords.words('portuguese')\n",
    "finder.apply_word_filter(lambda w: len(w) < 4 or w.lower() in ignored_words and w.lower() in termlist)\n",
    "finder.apply_freq_filter(5)\n",
    "tfinder.apply_word_filter(lambda w: len(w) < 4 or w.lower() in ignored_words and w.lower() in termlist)\n",
    "tfinder.apply_freq_filter(10)\n",
    "bilist = finder.nbest(bigram_measures.pmi, 200)\n",
    "trilist = tfinder.nbest(trigram_measures.pmi, 200)\n",
    "trilist = [\" \".join(t) for t in trilist]\n",
    "bilist = [\" \".join(b) for b in bilist]\n",
    "## Eliminates duplicate\n",
    "bilist = [b for b in bilist if all(b not in t for t in trilist)]\n",
    "\n",
    "tdf1 = pd.DataFrame(bilist)\n",
    "tdf2 = pd.DataFrame(trilist)\n",
    "\n",
    "tdf1.to_csv(\"data/TreatedTerms/bigrams.csv\")\n",
    "tdf2.to_csv(\"data/TreatedTerms/trigrams.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import Text\n",
    "from nltk import word_tokenize\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "\n",
    "quest = pd.read_csv('data/Raw/quinhentas_perguntas.csv')\n",
    "ignored_words = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "words = set()\n",
    "for idx, row in quest.iterrows():\n",
    "    words = words.union(set([w for w in word_tokenize(row[1]) if w not in ignored_words]))\n",
    "fulltext = \"\"\n",
    "termlist = pd.read_csv('/home/tiago_duque/Área de Trabalho/Dissertação/textO/data/TreatedTerms/tfidfterms.csv')\n",
    "termlist = list(termlist['0'])\n",
    "toremove = set()\n",
    "for word in words:\n",
    "    if word in termlist or lemmatizer(word, \"NOUN\")[0] in termlist or lemmatizer(word,\"VERB\")[0] in termlist:\n",
    "        toremove.add(word)\n",
    "for word in toremove:\n",
    "    words.remove(word)\n",
    "for idx, row in quest.iterrows():\n",
    "    fulltext+=row[1]\n",
    "tokens = word_tokenize(fulltext)\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "tfinder = TrigramCollocationFinder.from_words(tokens)\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "finder.apply_word_filter(lambda w: len(w) < 4 or w.lower() in ignored_words and w.lower() in words)\n",
    "finder.apply_freq_filter(3)\n",
    "tfinder.apply_word_filter(lambda w: len(w) < 4 or w.lower() in ignored_words and w.lower() in words)\n",
    "tfinder.apply_freq_filter(5)\n",
    "bilist = finder.nbest(bigram_measures.pmi, 200)\n",
    "trilist = tfinder.nbest(trigram_measures.pmi, 200)\n",
    "trilist = [\" \".join(t) for t in trilist]\n",
    "bilist = [\" \".join(b) for b in bilist]\n",
    "## Eliminates duplicate\n",
    "bilist = [b for b in bilist if all(b not in t for t in trilist)]\n",
    "words = [w for w in words if len(w) > 2]\n",
    "words = set([lemmatizer(w, \"NOUN\")[0] for w in words])\n",
    "words = set([lemmatizer(w, \"VERB\")[0] for w in words])\n",
    "df = pd.DataFrame({\"term\":list(words)})\n",
    "df.to_csv('data/TreatedTerms/wordsFromQuestions.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
